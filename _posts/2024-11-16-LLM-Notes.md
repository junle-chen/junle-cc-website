
### Configuring the Learning Rate

base_lr * sqrt(supervised_tokens_in_batch / pretrained_bsz)

假设在微调 Mistral 模型，以下是已知的条件：

- **base_lr**：5e-5（预训练 Mistral 模型的学习率）。
- 总监督性标记数：2百万个标记（2,000,000）。
- 批次大小为 1（即单卡训练）。
- 总训练步骤数为 350 步。
- 预训练时的批次大小：4,000,000（4百万）

![](../assets/img/Pasted%20image%2020241116222042.png)

### Gradient Accumulation
#### 1. **Global Batch 和 Mini-Batch**

- **Global Batch（全局批次）**：
    
    - 全局批次指的是一次完整的参数更新所涉及的所有样本数据。在深度学习中，由于硬件资源限制（如 GPU 内存有限），我们通常无法一次性加载所有的训练数据。
    - 因此，Global Batch 被分割成若干个 **Mini-Batch**，逐步在 GPU 上计算。
- **Mini-Batch（小批次）**：
    
    - Mini-Batch 是全局批次的一个子集。每次 GPU 会从全局批次中抽取一个小批次的数据进行训练。
    - 在图中，Global Batch 被分为 4 个 Mini-Batch（Mini-Batch 0、1、2 和 3）。

---

#### 2. **计算过程**

1. **Mini-Batch 的前向传播与反向传播**：
    
    - 每个 Mini-Batch 被依次送入 GPU。
    - GPU 对该 Mini-Batch 执行 **前向传播（Forward Pass）** 和 **反向传播（Backward Pass）**，计算出对应的梯度（Grad），如 `grad0`、`grad1` 等。
2. **累积梯度**：
    
    - 每个 Mini-Batch 计算的梯度被临时存储。
    - 这些梯度不会立即用于更新模型，而是等待所有 Mini-Batch 的梯度计算完成。
3. **Global Batch 梯度合并**：
    
    - 所有 Mini-Batch 的梯度（`grad0`、`grad1`、`grad2`、`grad3`）被合并，生成 **Global Batch 的梯度**。
    - 合并方法通常是对 Mini-Batch 梯度取平均值。
4. **参数更新**：
    
    - 在完成 Global Batch 梯度的计算后，使用这些梯度更新模型参数（例如通过优化器，如 SGD 或 Adam）。
    - 这种方式与直接每个 Mini-Batch 更新参数相比，通常更稳定，尤其是当全局批次大小较大时。


